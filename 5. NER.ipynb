{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1fa6b2-23eb-46a5-8c5d-b3ad9989bceb",
   "metadata": {},
   "source": [
    "# 5. Name Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4d199-7f2a-4735-bf9b-e1152f09db8c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c33b2f5a-6eb8-4718-91f2-fe848d13d3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --q transformers torch seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "628bb9c7-399d-4f63-9a94-fc49cbceb9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87602e84-6fde-48f7-a805-1dd4f6f96c61",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7e177c1-f9d5-4b10-a9f2-b9fb8c7a1342",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since conll2003 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'conll2003' at /home/jupyter/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/01ad4ad271976c5258b9ed9b910469a806ff3288 (last modified on Fri Feb  7 20:18:20 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset('conll2003')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37b18fe1-9eed-43bc-97ca-4cb0b08d14e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4f157-8292-40ed-94fb-4712657bb610",
   "metadata": {},
   "source": [
    "**Note** that punctuation is included in the list of words, and words haven't been low-cased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e11f0f57-d9bc-48bd-b9b2-935b6a5ca375",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a27ba-6df5-4136-aaae-83ec643a19c7",
   "metadata": {},
   "source": [
    "**Note** that POS tags, Chunk tags, and NER tags are all sequences, all with different classes and each class with string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be633b65-9a75-47dd-b7a2-a32b2f77082f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9a295-5499-47b1-9233-e4f340e45f90",
   "metadata": {},
   "source": [
    "This names property from the feature attribute returns a string list, which we can use later when we want to map label IDs back to label names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "675b31ab-452d-4a72-8fef-bf4550fd1482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save for later\n",
    "label_names = data['train'].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbd6bbb-855c-41df-8885-3eb6d214281a",
   "metadata": {},
   "source": [
    "## Data Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e335509-3adc-473d-8b1a-ae62f72e93a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We could also try using BERT directly\n",
    "checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594f4f8-e058-40b3-a7de-512505000588",
   "metadata": {},
   "source": [
    "If we want to tokenize a doc that has been already split into words, like this one, we just need to pass in the argument `is_split_into_words=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b9582b0-bcfc-42e2-80f7-97c5fedd9e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "t = tokenizer(data['train'][idx]['tokens'], is_split_into_words=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbd800c4-8968-4391-82ec-bd949f555cec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8297c8c-445f-47f2-a486-9f47a3549650",
   "metadata": {},
   "source": [
    "We see that although the tokenized data seemed like a dictionary, it's really a BatchEncoding object. This object has some useful object methods we can use, e.g. we can call tokens() to see the tokens represented in string format, instead of ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bc41c92-feb0-4676-8320-960543c8c1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5517e-c917-4ce9-aa0c-000284298cb1",
   "metadata": {},
   "source": [
    "We see the usual `[CLS]` and `[SEP]` [special tokens from BERT](https://www.neclab.eu/human-centric-ai-a-road-map-to-human-ai-collaboration/attending-to-future-tokens-for-bidirectional-sequence-generation#:~:text=The%20special%20tokens%20have%20specific,with%20the%20%5BMASK%5D%20token). Challenge: We have an input sequence and an output sequence, and their lengths vary. And we need to align the targets to the tokens, as tokens (words) have been split into sub-tokens (sub-words), but targets in NER are at the token (word) level.\n",
    "\n",
    "To fix this, we will **expand** our dataset, in the sense that for any word split into multiple tokens, we’ll assign the same target.\n",
    "\n",
    "But wait! What about special tokens from BERT (e.g. [CLS] and [SEP])? We need to create targets for those too.\n",
    "- Why do we need to do this? Transformers work that way. It’s similar to the RNN concept for the same task, we have (x(t)  h(t)  y(t), for all t), so we need all that tokens representation in their former and latter stage.\n",
    "- What value should we set for them? -100. Why? Because that’s the value that Hugging Face uses to know it needs to ignore them, not using them to update the model weights.\n",
    "\n",
    "We’ll need to write our own algorithm for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b334c0d0-edf1-4390-8f5b-e6e20c3179dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value of i indicates it's the i'th word in the input sentence (counting from 0)\n",
    "t.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cff62d67-00b8-4eb2-8723-19a3ad849c20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_ids()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec23706b-a67c-40bc-87a5-6aef014e8989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detail: Need to map the beginning and inside each word for different types of words\n",
    "#['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "begin2inside = {\n",
    "  1: 2,\n",
    "  3: 4,\n",
    "  5: 6,\n",
    "  7: 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74e40ce1-467a-4b28-9b7b-45605937eb36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_targets(labels, word_ids):\n",
    "    aligned_labels = []\n",
    "    last_word = None\n",
    "    for word in word_ids:\n",
    "        if word is None:         # token like [CLS]\n",
    "            label = -100\n",
    "        elif word != last_word:  # new word!\n",
    "            label = labels[word]\n",
    "        else:                    # same word as before\n",
    "            label = labels[word]\n",
    "\n",
    "            # change B-<tag> to I-<tag> if necessary\n",
    "            if label in begin2inside:\n",
    "                label = begin2inside[label]\n",
    "\n",
    "        # add the label \n",
    "        aligned_labels.append(label)\n",
    "\n",
    "        # update last word\n",
    "        last_word = word\n",
    "\n",
    "    return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfa86e8b-2e50-40a4-a720-40853a90612f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try our function\n",
    "labels = data['train'][idx]['ner_tags']\n",
    "word_ids = t.word_ids()\n",
    "aligned_targets = align_targets(labels, word_ids)\n",
    "aligned_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d48ef12d-5011-45ff-b9d6-759d8d53ed37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\tNone\n",
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "la\tO\n",
      "##mb\tO\n",
      ".\tO\n",
      "[SEP]\tNone\n"
     ]
    }
   ],
   "source": [
    "aligned_labels = [label_names[t] if t >= 0 else None for t in aligned_targets]\n",
    "for x, y in zip(t.tokens(), aligned_labels):\n",
    "    print(f\"{x}\\t{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15ccfe1a-b0dc-4dfe-b24e-224cee7fa34b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\tNone\n",
      "Ger\tB-MISC\n",
      "##man\tI-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "Micro\tB-ORG\n",
      "##soft\tI-ORG\n",
      "[SEP]\tNone\n"
     ]
    }
   ],
   "source": [
    "# make up a fake input just to test if B vs I works good\n",
    "words = [\n",
    "  '[CLS]', 'Ger', '##man', 'call', 'to', 'boycott', 'Micro', '##soft', '[SEP]']\n",
    "word_ids = [None, 0, 0, 1, 2, 3, 4, 4, None]\n",
    "labels = [7, 0, 0, 0, 3]\n",
    "aligned_targets = align_targets(labels, word_ids)\n",
    "aligned_labels = [label_names[t] if t >= 0 else None for t in aligned_targets]\n",
    "for x, y in zip(words, aligned_labels):\n",
    "  print(f\"{x}\\t{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1be7b2f0-328c-4421-9693-3748bac8cd0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize both inputs and targets\n",
    "def tokenize_fn(batch):\n",
    "    # tokenize the input sequence first\n",
    "    # this populates input_ids, attention_mask, etc.\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch['tokens'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels_batch = batch['ner_tags'] # original targets\n",
    "    aligned_labels_batch = []\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        aligned_labels_batch.append(align_targets(labels, word_ids))\n",
    "\n",
    "    # recall: the 'target' must be stored in key called 'labels'\n",
    "    tokenized_inputs['labels'] = aligned_labels_batch\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75ceb3-555b-45f4-b777-80640d2883ba",
   "metadata": {},
   "source": [
    "The former function returns input our tokenized inputs, now containing their ids, attention masks, and so forth, but also the aligned labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81edc16a-fbf1-44a9-ba63-447341025540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# want to remove these from model inputs - they are neither inputs nor targets\n",
    "data[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0459d2e-a339-431a-a45e-eee98fb3e693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = data.map(\n",
    "  tokenize_fn,\n",
    "  batched=True,\n",
    "  remove_columns=data[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9c18ab8-55fe-48e8-921b-46e098acbaab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e6c15-83c1-4722-ac1b-b6b1e1d5e846",
   "metadata": {},
   "source": [
    "As we see, columns in our dataset are the ones we need now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7264da03-94b6-448e-800d-abe193e2b75f",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "258a0767-d829-4874-8e4c-3d569e7f5ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2aa01733-64e5-47ec-8ebf-e08fbd4d227d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101,\n",
       "   7270,\n",
       "   22961,\n",
       "   1528,\n",
       "   1840,\n",
       "   1106,\n",
       "   21423,\n",
       "   1418,\n",
       "   2495,\n",
       "   12913,\n",
       "   119,\n",
       "   102],\n",
       "  [101, 1943, 14428, 102]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]],\n",
       " 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100], [-100, 1, 2, -100]]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568fdb8e-1a9d-4665-afaf-78f51656cf6e",
   "metadata": {},
   "source": [
    "Unfortunately, this dict is not a format that Data Collator can work with. We need to grab a dict for each sample separately, and store it in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ef3f8a1-9607-45cc-aba1-fea94dcc7bef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [101,\n",
       "   7270,\n",
       "   22961,\n",
       "   1528,\n",
       "   1840,\n",
       "   1106,\n",
       "   21423,\n",
       "   1418,\n",
       "   2495,\n",
       "   12913,\n",
       "   119,\n",
       "   102],\n",
       "  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]},\n",
       " {'input_ids': [101, 1943, 14428, 102],\n",
       "  'attention_mask': [1, 1, 1, 1],\n",
       "  'labels': [-100, 1, 2, -100]}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenized_datasets['train'][i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a31771bf-6d7b-4fee-b1c8-3ef8be7595ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "batch['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58432804-733a-471c-bf52-84e9384ebaa1",
   "metadata": {},
   "source": [
    "**Note** that we get back a Torch tensor object, with our words expanded, and with pad tokens set to -100. There pad tokens being set correctly could trick our accuracy, giving a false sensation that our model is performing great, so we'll need to take this into account in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff3698-bf3e-48b5-8278-28c4358cb0ee",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "765e2702-0b89-4e4b-ac4e-70f524b4bdd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52affcd1-6930-44c0-b674-2d6c0215cc88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall_precision': 0.0,\n",
       " 'overall_recall': 0.0,\n",
       " 'overall_f1': 0.0,\n",
       " 'overall_accuracy': 0.6666666666666666}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out. This metric assumes we're working with batches of sentences (we should pass a list of lists)\n",
    "metric.compute(\n",
    "    predictions=[[0, 0, 0]],\n",
    "    references=[[0, 0, 1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1bcf3491-664c-4fc9-8797-e8bf62fcd817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'overall_precision': 0.0,\n",
       " 'overall_recall': 0.0,\n",
       " 'overall_f1': 0.0,\n",
       " 'overall_accuracy': 0.6666666666666666}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out. This metric assumes we're working with batches of sentences (we should pass a list of lists)\n",
    "metric.compute(\n",
    "    predictions=[['A','A','A']],\n",
    "    references=[['A','A','B']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d5752c01-0164-468f-b758-cadaa44ccf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'overall_precision': 0.5,\n",
       " 'overall_recall': 0.5,\n",
       " 'overall_f1': 0.5,\n",
       " 'overall_accuracy': 0.75}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out, right formats now\n",
    "metric.compute(\n",
    "    predictions=[['O', 'O', 'I-ORG', 'B-MISC']],\n",
    "    references=[['O', 'B-ORG', 'I-ORG', 'B-MISC']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f0b4e-9960-4373-a431-c0b1352cb47e",
   "metadata": {},
   "source": [
    "**Note that** we get metrics for each entity, and then overall metrics. \n",
    "**Note 2:** It seems off that ORG precision is 0, even if we did some correct predictions (it should intuitively be 0.5). This is because seqeval does some special computation for ER evaluation (so their evaluation differs from the generic ML one).\n",
    "\n",
    "Let's now write our compute metrics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d03c651c-64a6-44fb-a333-96ba05833797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # remove -100 from labels and predictions\n",
    "    # and convert the label_ids to label names\n",
    "    str_labels = [\n",
    "        [label_names[t] for t in label if t != -100] for label in labels\n",
    "    ]\n",
    "\n",
    "    # do the same for predictions whenever true label is -100\n",
    "    str_preds = [\n",
    "        [label_names[p] for p, t in zip(pred, targ) if t != -100] \\\n",
    "            for pred, targ in zip(preds, labels)\n",
    "    ]\n",
    "\n",
    "    the_metrics = metric.compute(predictions=str_preds, references=str_labels)\n",
    "    return {\n",
    "        'precision': the_metrics['overall_precision'],\n",
    "        'recall': the_metrics['overall_recall'],\n",
    "        'f1': the_metrics['overall_f1'],\n",
    "        'accuracy': the_metrics['overall_accuracy'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ad633-58fa-4221-be04-acea2f019279",
   "metadata": {},
   "source": [
    "## Model and trainer\n",
    "\n",
    "We'll use AutoModelForTokenClassification, which is accurate for our current task. For this, we need to specify the labels in an alternative way, passing the arguments id2label and label2id, defined based on our input dataset. Reminder:\n",
    "- Our `label_names` var is a list which already stores a mapping from id to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3336dc90-c517-4049-923d-5c1da7c3bc48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label = {k:v for k, v in enumerate(label_names)}\n",
    "label2id = {v:k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8dc8f74-4a8c-4907-92c0-a2618349554a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbd21b00-8e2c-4062-ba37-b348a5a3530f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"distilbert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0446b-be3d-4fb3-843a-8e9492f10e75",
   "metadata": {},
   "source": [
    "**NOTE:** Having an accelerator error I was not able to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ce030d40-f1c5-4b8a-bf30-2e07f78ed8f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 05:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>0.086093</td>\n",
       "      <td>0.877898</td>\n",
       "      <td>0.911141</td>\n",
       "      <td>0.894211</td>\n",
       "      <td>0.976291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.075361</td>\n",
       "      <td>0.908852</td>\n",
       "      <td>0.931336</td>\n",
       "      <td>0.919957</td>\n",
       "      <td>0.981236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.070182</td>\n",
       "      <td>0.918950</td>\n",
       "      <td>0.936890</td>\n",
       "      <td>0.927833</td>\n",
       "      <td>0.982325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory distilbert-finetuned-ner/checkpoint-1756 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory distilbert-finetuned-ner/checkpoint-3512 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory distilbert-finetuned-ner/checkpoint-5268 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.08099188565664792, metrics={'train_runtime': 314.6155, 'train_samples_per_second': 133.887, 'train_steps_per_second': 16.744, 'total_flos': 460336113849150.0, 'train_loss': 0.08099188565664792, 'epoch': 3.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "185f962d-cfdf-4f03-8992-c1c6c010f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('my_saved_model') # Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fb79b818-b835-4295-b5fd-e8b155746b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\n",
    "  \"token-classification\",\n",
    "  model='my_saved_model',\n",
    "  aggregation_strategy=\"simple\",\n",
    "  device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9f881c1-5ec8-4738-958f-8c331c79ef84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99922806,\n",
       "  'word': 'Bill Gates',\n",
       "  'start': 0,\n",
       "  'end': 10},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9982521,\n",
       "  'word': 'Microsoft',\n",
       "  'start': 26,\n",
       "  'end': 35},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99892324,\n",
       "  'word': 'Seattle',\n",
       "  'start': 39,\n",
       "  'end': 46},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.997781,\n",
       "  'word': 'Washington',\n",
       "  'start': 48,\n",
       "  'end': 58}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Bill Gates was the CEO of Microsoft in Seattle, Washington.\"\n",
    "ner(s)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
