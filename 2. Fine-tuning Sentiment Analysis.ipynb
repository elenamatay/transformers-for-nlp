{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8cbb962-ff82-4a89-bcae-bd111793058a",
   "metadata": {},
   "source": [
    "# 2. Fine-tuning Sentiment Analysis\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9450d9d7-93e2-4a64-b4a6-fce3c0eb2b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --q transformers torch datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "187828b5-b6c7-4b37-97e0-37f4676b3d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925746c-9c25-456d-841c-d72bd5f230d6",
   "metadata": {},
   "source": [
    "**Note:** This needs some later clean-up and adding, as the instructor imports things as we are needing them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccbb49-89d4-4458-a854-74e959fcc867",
   "metadata": {},
   "source": [
    "## Explore data in GLUE benchmark for SST-2 task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6825b181-1680-4721-a074-ae6346c435a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'sst2' at /home/jupyter/.cache/huggingface/datasets/glue/sst2/1.0.0/fd8e86499fa5c264fcaad392a8f49ddf58bf4037 (last modified on Mon Dec 25 20:15:54 2023).\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/amazon_polarity\n",
    "# Larger dataset that takes a long time to process, we may want to try it later\n",
    "# dataset = load_dataset(\"amazon_polarity\")\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67f700c-9fb8-4396-aeb8-0f3dc479090b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'sst2' at /home/jupyter/.cache/huggingface/datasets/glue/sst2/1.0.0/fd8e86499fa5c264fcaad392a8f49ddf58bf4037 (last modified on Mon Dec 25 20:15:54 2023).\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67608f6d-6216-4b3b-bf8b-023353406ca9",
   "metadata": {},
   "source": [
    "We have 3 dataset objects, which are Python dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01b8138-b7bb-4a95-80cb-3d1a17cab479",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c506737-aa9c-4aaa-b00b-49c8bfca3b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', 'idx'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'] # Index dataset using its key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "240f03df-5a0e-453a-b6de-b2941cc47887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_TF_DATASET_REFS',\n",
       " '__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getitems__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_build_local_temp_path',\n",
       " '_check_index_is_initialized',\n",
       " '_data',\n",
       " '_estimate_nbytes',\n",
       " '_fingerprint',\n",
       " '_format_columns',\n",
       " '_format_kwargs',\n",
       " '_format_type',\n",
       " '_generate_tables_from_cache_file',\n",
       " '_generate_tables_from_shards',\n",
       " '_get_cache_file_path',\n",
       " '_get_output_signature',\n",
       " '_getitem',\n",
       " '_indexes',\n",
       " '_indices',\n",
       " '_info',\n",
       " '_map_single',\n",
       " '_new_dataset_with_indices',\n",
       " '_output_all_columns',\n",
       " '_push_parquet_shards_to_hub',\n",
       " '_save_to_disk_single',\n",
       " '_select_contiguous',\n",
       " '_select_with_indices_mapping',\n",
       " '_split',\n",
       " 'add_column',\n",
       " 'add_elasticsearch_index',\n",
       " 'add_faiss_index',\n",
       " 'add_faiss_index_from_external_arrays',\n",
       " 'add_item',\n",
       " 'align_labels_with_mapping',\n",
       " 'builder_name',\n",
       " 'cache_files',\n",
       " 'cast',\n",
       " 'cast_column',\n",
       " 'citation',\n",
       " 'class_encode_column',\n",
       " 'cleanup_cache_files',\n",
       " 'column_names',\n",
       " 'config_name',\n",
       " 'data',\n",
       " 'dataset_size',\n",
       " 'description',\n",
       " 'download_checksums',\n",
       " 'download_size',\n",
       " 'drop_index',\n",
       " 'export',\n",
       " 'features',\n",
       " 'filter',\n",
       " 'flatten',\n",
       " 'flatten_indices',\n",
       " 'format',\n",
       " 'formatted_as',\n",
       " 'from_buffer',\n",
       " 'from_csv',\n",
       " 'from_dict',\n",
       " 'from_file',\n",
       " 'from_generator',\n",
       " 'from_json',\n",
       " 'from_list',\n",
       " 'from_pandas',\n",
       " 'from_parquet',\n",
       " 'from_spark',\n",
       " 'from_sql',\n",
       " 'from_text',\n",
       " 'get_index',\n",
       " 'get_nearest_examples',\n",
       " 'get_nearest_examples_batch',\n",
       " 'homepage',\n",
       " 'info',\n",
       " 'is_index_initialized',\n",
       " 'iter',\n",
       " 'license',\n",
       " 'list_indexes',\n",
       " 'load_elasticsearch_index',\n",
       " 'load_faiss_index',\n",
       " 'load_from_disk',\n",
       " 'map',\n",
       " 'num_columns',\n",
       " 'num_rows',\n",
       " 'prepare_for_task',\n",
       " 'push_to_hub',\n",
       " 'remove_columns',\n",
       " 'rename_column',\n",
       " 'rename_columns',\n",
       " 'reset_format',\n",
       " 'save_faiss_index',\n",
       " 'save_to_disk',\n",
       " 'search',\n",
       " 'search_batch',\n",
       " 'select',\n",
       " 'select_columns',\n",
       " 'set_format',\n",
       " 'set_transform',\n",
       " 'shape',\n",
       " 'shard',\n",
       " 'shuffle',\n",
       " 'size_in_bytes',\n",
       " 'sort',\n",
       " 'split',\n",
       " 'supervised_keys',\n",
       " 'task_templates',\n",
       " 'to_csv',\n",
       " 'to_dict',\n",
       " 'to_iterable_dataset',\n",
       " 'to_json',\n",
       " 'to_list',\n",
       " 'to_pandas',\n",
       " 'to_parquet',\n",
       " 'to_sql',\n",
       " 'to_tf_dataset',\n",
       " 'train_test_split',\n",
       " 'unique',\n",
       " 'version',\n",
       " 'with_format',\n",
       " 'with_transform']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(raw_datasets['train']) # Let's see what attributes and methods the object has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f258d53-1570-4756-9518-b86aa5f75f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c510ed-f043-4c21-a5d5-ef78de3555c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "sentence: string\n",
       "label: int64\n",
       "idx: int32\n",
       "----\n",
       "sentence: [[\"hide new secretions from the parental units \",\"contains no wit , only labored gags \",\"that loves its characters and communicates something rather beautiful about human nature \",\"remains utterly satisfied to remain the same throughout \",\"on the worst revenge-of-the-nerds clich√©s the filmmakers could dredge up \",...,\"you wish you were at home watching that movie instead of in the theater watching this one \",\"'s no point in extracting the bare bones of byatt 's plot for purposes of bland hollywood romance \",\"underdeveloped \",\"the jokes are flat \",\"a heartening tale of small victories \"],[\"suspense , intriguing characters and bizarre bank robberies , \",\"a gritty police thriller with all the dysfunctional family dynamics one could wish for \",\"with a wonderful ensemble cast of characters that bring the routine day to day struggles of the working class to life \",\"nonetheless appreciates the art and reveals a music scene that transcends culture and race . \",\"do we really need the tiger beat version ? \",...,\"when there 's nothing else happening \",\"on cable \",\"it with ring , \",\"far from a groundbreaking endeavor \",\"that these women are spectacular \"],...,[\"it does turn out to be a bit of a cheat in the end \",\"may be convinced that he has something significant to say \",\"to be both hugely entertaining and uplifting . \",\", boredom never takes hold . \",\"left to work with , sort of like michael jackson 's nose \",...,\"from a severe case of hollywood-itis \",\"the very best of them \",\"thrills , \",\"'s attracting audiences to unfaithful \",\"impressively delicate range \"],[\"starts off promisingly but then proceeds to flop \",\"distinguished actor \",\"on their parents ' anguish \",\"pays off and is effective if you stick with it \",\"is n't particularly funny \",...,\"a delightful comedy \",\"anguish , anger and frustration \",\"at achieving the modest , crowd-pleasing goals it sets for itself \",\"a patient viewer \",\"this new jangle of noise , mayhem and stupidity must be a serious contender for the title . \"]]\n",
       "label: [[0,0,1,0,0,...,0,0,0,0,1],[1,1,1,1,0,...,0,0,1,0,1],...,[0,0,1,1,0,...,0,1,1,1,1],[0,1,0,1,0,...,1,0,1,1,0]]\n",
       "idx: [[0,1,2,3,4,...,995,996,997,998,999],[1000,1001,1002,1003,1004,...,1995,1996,1997,1998,1999],...,[66000,66001,66002,66003,66004,...,66995,66996,66997,66998,66999],[67000,67001,67002,67003,67004,...,67344,67345,67346,67347,67348]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ebe2f9d-65b9-4893-aef8-c87860f98c45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31803a02-c4b0-4c0e-8018-fc1bf514c080",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['glow ',\n",
       "  'a classical dramatic animated feature ',\n",
       "  'best espionage picture '],\n",
       " 'label': [1, 1, 1],\n",
       " 'idx': [50000, 50001, 50002]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][50000:50003]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a64177-ceb2-4165-90ae-f18a5798e4f2",
   "metadata": {},
   "source": [
    "Note that the result is a dictionary of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f28e80a-a378-4b9e-a641-02ce6739672a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_datasets['train'][50000:50003])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09bcc3-d4e8-4419-b8b2-8737bd254874",
   "metadata": {},
   "source": [
    "Let's check the features attribute now, which shows even the name of the classes in the labels feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f36049-7c34-4a19-8999-e6e91b3a54dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'positive'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca788daa-ef30-4461-865b-d0ec5146d69f",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f95036-d908-42ba-b406-0a49bd3947e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2479b55b-d60c-4f75-9cd0-d460efff86d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1162d2775a486fadc07e8ad8473bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased\" # we chose this model checkpoint as it trains faster, but \"bert-base-uncased\" could've been a good option too\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03ef4e67-a1a1-404c-93a1-28bf6d5eafa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102],\n",
      "               [101,\n",
      "                3397,\n",
      "                2053,\n",
      "                15966,\n",
      "                1010,\n",
      "                2069,\n",
      "                4450,\n",
      "                2098,\n",
      "                18201,\n",
      "                2015,\n",
      "                102],\n",
      "               [101,\n",
      "                2008,\n",
      "                7459,\n",
      "                2049,\n",
      "                3494,\n",
      "                1998,\n",
      "                10639,\n",
      "                2015,\n",
      "                2242,\n",
      "                2738,\n",
      "                3376,\n",
      "                2055,\n",
      "                2529,\n",
      "                3267,\n",
      "                102]]}\n"
     ]
    }
   ],
   "source": [
    "# Test our tokenizer on a subsample of our sentences\n",
    "tokenized_sentences = tokenizer(raw_datasets['train'][0:3]['sentence'])\n",
    "from pprint import pprint\n",
    "pprint(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1747e25b-b447-444c-a363-e8809d749871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrap out tokenizer with a new function to set truncation to True\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd2c7383-19c2-46ed-9237-0ca1472ddc75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map the wrapper function to all our samples\n",
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "089bcd6a-6c4c-4ec7-b450-4a6699f0288f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6e29f52-e076-42a2-8846-2efb10b5fbe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define training arguments object, which starts overfitting with just a few epochs\n",
    "training_args = TrainingArguments(\n",
    "    'my_trainer',\n",
    "    evaluation_strategy='epoch', # Required if evaluating on a provided dataset\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec2034b7-b842-49d1-885b-7358c1575a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04fbbff2-42c1-4d9b-bd63-b255824e1c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8064f2a-9992-4c7b-b6f2-cecedfaa6bf4",
   "metadata": {},
   "source": [
    "**Note** the expected warning, as we do intend to train the last layers of our model (therefore update the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cc9c3bc-6b0a-4b0b-b329-b7fc1b946178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60952b89-1a0a-40d9-8d6e-83a1c4a1cf7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5bed6db-7fff-4a83-9d61-c61bff16833e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.10/site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "976c9ea8-3f7b-4502-a420-c15f56d94f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "‚îú‚îÄDistilBertModel: 1-1                                  --\n",
       "‚îÇ    ‚îî‚îÄEmbeddings: 2-1                                  --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-1                              23,440,896\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-2                              393,216\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-3                              1,536\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-4                                --\n",
       "‚îÇ    ‚îî‚îÄTransformer: 2-2                                 --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-5                             42,527,232\n",
       "‚îú‚îÄLinear: 1-2                                           590,592\n",
       "‚îú‚îÄLinear: 1-3                                           1,538\n",
       "‚îú‚îÄDropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef91ec-f6d4-41d5-b16c-a6cf7f23f0cc",
   "metadata": {},
   "source": [
    "**Note:** Pytorch doesn't show by default the input and output layers size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d49d9965-9c3a-4fe9-841a-662eb311de55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's do a sanity check to see if we end up training all the weights of the NN, and not just the ones\n",
    "# on the newly trained layers. For doing this comparison, let's save the model params before fine-tuning.\n",
    "params_before = []\n",
    "for name, p in model.named_parameters():\n",
    "    params_before.append(p.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df317fe8-a9ee-4268-91dc-fe8b0b5bcad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_7541/1842790964.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"sst2\")\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95390cd0-5196-432b-b390-cc1d947996a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=[1, 0, 1], references=[1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94c0a502-710c-43a9-851f-86b0cb9694bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(logits_and_labels):\n",
    "    # metric = load_metric(\"glue\", \"sst2\")\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "516b0db7-622d-4331-a46b-e41f693bfb59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8592fa62-56e9-46fe-9edd-ebdb2cf7b58e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8419' max='8419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8419/8419 06:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.345907</td>\n",
       "      <td>0.902523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8419, training_loss=0.2634747613140432, metrics={'train_runtime': 398.8511, 'train_samples_per_second': 168.857, 'train_steps_per_second': 21.108, 'total_flos': 517212489917652.0, 'train_loss': 0.2634747613140432, 'epoch': 1.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bcf607f-772a-4b1b-a693-3a2b9bc612fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model('sa_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cb6c94c-579e-4379-9cc0-3a4b3da5e03f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1 - Named Entity Recognition (NER).ipynb'\n",
      "'1 - Pipeline Mask Language Modeling.ipynb'\n",
      "'1 - Question Answering.ipynb'\n",
      "'1 - Sentiment analysis.ipynb'\n",
      "'1 - Text Generation.ipynb'\n",
      "'1 - Text summarization.ipynb'\n",
      "'1 - Translation.ipynb'\n",
      "'1 - Zero-shot classification.ipynb'\n",
      "'2 - Fine-tuning Sentiment Analysis.ipynb'\n",
      "'2 - Fine-tuning sentiment custom dataset.ipynb'\n",
      "'2 - Models & Tokenizers.ipynb'\n",
      "'2 - NER.ipynb'\n",
      "'2 - POS Tagger.ipynb'\n",
      "'2 - QA - Advanced.ipynb'\n",
      " AirlineTweets.csv\n",
      "'Fine-Tuning RTE.ipynb'\n",
      " README.md\n",
      " bbc_text_cls.csv\n",
      " data.json\n",
      " data_sentiment_finetuning.csv\n",
      " my_trainer\n",
      " ner_test.pkl\n",
      " ner_train.pkl\n",
      " robert_frost.txt\n",
      " sa_saved_model\n",
      " spa-eng\n",
      " spa-eng.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04be3949-5e08-42e5-a9f5-c49ad5040338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t   special_tokens_map.json  tokenizer_config.json  vocab.txt\n",
      "model.safetensors  tokenizer.json\t    training_args.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls sa_saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18c441ca-0d6c-49d5-8fa9-85369cca6817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c08663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_sa_model = pipeline('text-classification', model='sa_saved_model', device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc43367-a094-4e59-b5d9-c6c2634d6af4",
   "metadata": {},
   "source": [
    "Let's now try our new Sentiment Analysis model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11c874c1-27dc-4db0-ba7e-8a0c23473f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9996929168701172}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sa_model('This movie is great!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd2342b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9623615145683289}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sa_model('I kind of liked this movie, but I did not love the main character actress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1c0c6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9623615145683289}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sa_model('I kind of liked this movie, but I did not love the main character actress')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
