# transformers-for-nlp

In [this course](https://www.udemy.com/course/data-science-transformers-nlp/), I learned the foundations and practical applications of transformers in NLP. The course was divided into three major parts: Using Transformers, Fine-Tuning Transformers, and Transformers In-Depth.

In the first part, I explored how to use pre-trained transformers for various tasks such as text classification, named entity recognition, text summarization, machine translation, question-answering, and zero-shot classification.

In the second part (the one showcased in this Github repo), I learned how to fine-tune transformers on custom datasets to improve their performance on specific tasks like sentiment analysis and spam detection.

The final part provided an in-depth understanding of the inner workings of transformers, including models like BERT, GPT, GPT-2, GPT-3, and GPT-4.
This comprehensive course equipped me with both practical skills and theoretical knowledge to leverage transformers effectively in real-world applications.
