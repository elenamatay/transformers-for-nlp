{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ca03c6-d57b-4112-8e44-5593e1fc9b30",
   "metadata": {},
   "source": [
    "# 4. Fine tuning a Text Classification model with Multiple Input Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a728fe-804f-4d64-9dac-5bcb397b8af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --q transformers torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba81ade-dcd4-4f4b-8e3c-1b112203b709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "876631d7-475e-4e40-bdd7-eaa397a5f67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'rte' at /home/jupyter/.cache/huggingface/datasets/glue/rte/1.0.0/fd8e86499fa5c264fcaad392a8f49ddf58bf4037 (last modified on Fri Feb  7 19:44:00 2025).\n"
     ]
    }
   ],
   "source": [
    "# The Recognizing Textual Entailment (RTE) datasets come from a series of annual\n",
    "# textual entailment challenges. We combine the data from RTE1 (Dagan et al.,\n",
    "# 2006), RTE2 (Bar Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5\n",
    "# (Bentivogli et al., 2009).4 Examples are constructed based on news and\n",
    "# Wikipedia text. We convert all datasets to a two-class split, where for\n",
    "# three-class datasets we collapse neutral and contradiction into not\n",
    "# entailment, for consistency.\n",
    "raw_datasets = load_dataset(\"glue\", \"rte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4929148-6adb-4e44-9772-8837c3f04f84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 2490\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 277\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22fc55fb-522d-4453-8c4f-f136b9adcb4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['entailment', 'not_entailment'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7e5a9b-36de-41fa-967b-bdee785800c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No Weapons of Mass Destruction Found in Iraq Yet.',\n",
       " 'A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.',\n",
       " 'Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.',\n",
       " 'Judie Vivian, chief executive at ProMedica, a medical service company that helps sustain the 2-year-old Vietnam Heart Institute in Ho Chi Minh City (formerly Saigon), said that so far about 1,500 children have received treatment.',\n",
       " \"A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to be featured on BBC One's Crimewatch. Colette Aram, 16, was walking to her boyfriend's house in Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later.\",\n",
       " 'Britain said, Friday, that it has barred cleric, Omar Bakri, from returning to the country from Lebanon, where he was released by police after being detained for 24 hours.',\n",
       " \"Nearly 4 million children who have at least one parent who entered the U.S. illegally were born in the United States and are U.S. citizens as a result, according to the study conducted by the Pew Hispanic Center. That's about three quarters of the estimated 5.5 million children of illegal immigrants inside the United States, according to the study. About 1.8 million children of undocumented immigrants live in poverty, the study found.\",\n",
       " 'Like the United States, U.N. officials are also dismayed that Aristide killed a conference called by Prime Minister Robert Malval in Port-au-Prince in hopes of bringing all the feuding parties together.',\n",
       " \"WASHINGTON --  A newly declassified narrative of the Bush administration's advice to the CIA on harsh interrogations shows that the small group of Justice Department lawyers who wrote memos authorizing controversial interrogation techniques were operating not on their own but with direction from top administration officials, including then-Vice President Dick Cheney and national security adviser Condoleezza Rice. At the same time, the narrative suggests that then-Defense Secretary Donald H. Rumsfeld and then-Secretary of State Colin Powell were largely left out of the decision-making process.\",\n",
       " 'Only a week after it had no comment on upping the storage capacity of its Hotmail e-mail service, Microsoft early Thursday announced it was boosting the allowance to 250MB to follow similar moves by rivals such as Google, Yahoo, and Lycos.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['sentence1'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46637fdf-c109-469a-bccb-83b67525a3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-cased'\n",
    "# checkpoint = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d86a68-addd-49e7-a996-796c700ec84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee196dc-3505-4e45-bb70-06aacfa60a71",
   "metadata": {},
   "source": [
    "Test our tokenizer on the first pair of sentences in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "924dc27b-cb16-4f0c-9bb1-abdf8d03d2eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1302, 20263, 1104, 8718, 14177, 17993, 17107, 1107, 5008, 6355, 119, 102, 20263, 1104, 8718, 14177, 17993, 17107, 1107, 5008, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    raw_datasets['train']['sentence1'][0],\n",
    "    raw_datasets['train']['sentence2'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c17b34-c622-4e90-8bdf-de171ca0cff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed929d7f-4890-437c-849c-bde1704ae7b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distilbert doesn't use token_type_ids\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd365833-2562-43f3-8399-f4312f7a7afe",
   "metadata": {},
   "source": [
    "Decode the input IDs, we'll see our input sentences concatenated into a string, separated by [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bc2c08b-d700-4e4b-8ffd-eb6eeaaccd29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] No Weapons of Mass Destruction Found in Iraq Yet. [SEP] Weapons of Mass Destruction Found in Iraq. [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b4607f9-41b4-406e-98ff-00e1f913fff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b98c731d-b791-44cc-bfc0-948b1031f7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir='training_dir',\n",
    "  evaluation_strategy='epoch',\n",
    "  save_strategy='epoch',\n",
    "  num_train_epochs=5,\n",
    "  per_device_train_batch_size=16,\n",
    "  per_device_eval_batch_size=64,\n",
    "  logging_steps=150, # otherwise, 'no log' will appear under training loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d90c0e-1c52-4a61-851f-760c635a9460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2682/652322029.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"rte\")\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"glue\", \"rte\")\n",
    "metric.compute(predictions=[1, 0, 1], references=[1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9799a01-00bf-400d-8592-b7ccbd50b6f6",
   "metadata": {},
   "source": [
    "We only get the accuracy, so let's import F1 from scikit-learn and compute our metrics by defining a `compute_metrics` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a37d98a9-99b1-47eb-9f05-6207d78cca02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = np.mean(predictions == labels)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ccb7e-77b0-4899-a048-36427c004029",
   "metadata": {},
   "source": [
    "Now, let's define our tokenizer function. As always, the input to this function is a batch of data from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ea50429-b6a8-4267-947a-438cdff7a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['sentence1'], batch['sentence2'], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a088d61-b293-4b32-b28a-2d186bfb32b2",
   "metadata": {},
   "source": [
    "Now let's create our tokenized dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6a1983d-e5c5-48d3-90b0-23a78cf187b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c31e44ee95b4a568fe456c4d646b0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/277 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a29657-69a5-4854-90a5-3b63fce0b1da",
   "metadata": {},
   "source": [
    "Now let's create our trainer object and train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a434ddf4-0dc5-41aa-9320-f4da5c24bd4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 03:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>0.692890</td>\n",
       "      <td>0.523466</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.738031</td>\n",
       "      <td>0.498195</td>\n",
       "      <td>0.603989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.845680</td>\n",
       "      <td>0.552347</td>\n",
       "      <td>0.465517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>1.328066</td>\n",
       "      <td>0.584838</td>\n",
       "      <td>0.572491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>1.827858</td>\n",
       "      <td>0.563177</td>\n",
       "      <td>0.550186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=780, training_loss=0.4446157678579673, metrics={'train_runtime': 195.568, 'train_samples_per_second': 63.661, 'train_steps_per_second': 3.988, 'total_flos': 543824207151168.0, 'train_loss': 0.4446157678579673, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02c16d-d620-4f28-85d5-5835aa40ab82",
   "metadata": {},
   "source": [
    "As we can see, our model overfits already since epoch 2, so in the real world we should work on this. However, for educational purposes, let's just ignore this and make some predictions with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70cdb4ad-ec2e-4b62-be7c-c58de63b9a72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model('my_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7043a94-4864-4bd9-8723-f50dff36e14e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "p = pipeline('text-classification', model='my_saved_model', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a091d104-03c1-4d71-8e86-1f852fca6b37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'LABEL_1', 'score': 0.7081083655357361}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p({'text': 'I went to the store', 'text_pair': 'I am a bird'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efca3a1b-963a-4c2b-80d8-53ba9c2f2e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'LABEL_1', 'score': 0.9929063320159912}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p({'text': 'Elena got the job :)', 'text_pair': 'A stalker got here to try to understand if Elena deserved the job ;)'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef0c6b-73db-4913-abf1-a76a35372e9a",
   "metadata": {},
   "source": [
    "**Conclusion:** As the \"entailment\" definition explains:\n",
    "\n",
    "_Sentence A entails Sentence B if, necessarily, whenever Sentence A is true, Sentence B must also be true.  It's a strong logical connection.  It's not just that B is likely to be true if A is true; it must be true._\n",
    "\n",
    "We've got a 0.99 score on the last pair of sentences... I'll leave it to the reader to conclude!!!"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
