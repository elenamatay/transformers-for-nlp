{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58603cc4-0f32-46e6-b31b-80453a74d6ce",
   "metadata": {},
   "source": [
    "# 1. Models & Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6964bea-e4d2-460c-8383-8bede026774f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "021bec23-896e-4161-bbf3-5e5a2fd1eef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d61bc3-6207-4d9d-ad37-66f4ad35d589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40297a95803a4fe684c37f50ddaa3c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7016179285e74fc08babada66b0747f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01958e01d91c43f187f683568d59844f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9c1f38721342b4b7728183ebd99386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e013ad-6d0c-4c56-9626-7f950b0bdb0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc9d09c-83da-4c9b-8b6e-6c5614f184f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b874664e-b161-4eb4-8e51-06c4547acd87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Hello World\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "814efbb5-2251-4db0-9fcd-f9221f1a0e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7592, 2088]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4662aae-3d39-4ab4-8015-644d02c2b1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b0503ee-896d-4b30-b925-38ca7e735202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914370c7-9cfe-46b0-ac73-940bb921b606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7592, 2088, 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"hello world\")\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bcaf08a-61bf-4877-848f-fbc087d5f4dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'hello', 'world', '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68e64c-767e-4bc9-8985-a3c04e169952",
   "metadata": {},
   "source": [
    "We see the tokenizer adds two BERT special tokens: `[CLS]` and `[SEP]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92fb7173-ef8a-4a1e-9426-82bf35cc1933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello world [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ce63ec0-1a37-44b8-8401-955274649ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer(\"hello world\")\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d94a805-b2fe-48ac-8b05-b7dfdf45480d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2066, 8870, 1012, 102], [101, 2079, 2017, 2066, 8870, 2205, 1029, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize multiple sentences at once:\n",
    "data= [\n",
    "    \"I like cats.\",\n",
    "    \"Do you like cats too?\",\n",
    "]\n",
    "tokenizer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f7bb421-2565-4693-b623-9503271dab77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d842b488b3ec45f9a08f50ff856fd795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9e0b642-e53d-4ceb-8f3a-366b8cdbf980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[0.4758, 0.5030]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer(\"hello world\", return_tensors='pt')\n",
    "\n",
    "outputs = model(**model_inputs) # as using a PyTorch model, it just accepts torch tensors as inputs\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09db7fd-b482-4914-8a68-521532a2d0e1",
   "metadata": {},
   "source": [
    "The default was to create a binary classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5875c452-8d27-4b42-a299-acaa97492217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "567bf7a5-8c9e-445c-86d5-a848f374bd95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3364,  0.5550, -0.1813]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**model_inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd64a5-c004-438a-a3f9-e9402a1a952e",
   "metadata": {},
   "source": [
    "## Few ways we can access logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1a68a50-88c5-4ff8-85f3-da116accbea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3364,  0.5550, -0.1813]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef7d24bd-54f8-49f9-98d1-8e1692b2cd32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3364,  0.5550, -0.1813]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f938440-0d62-417c-b7ad-88789a3e3938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33635485,  0.55498725, -0.18131647]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.detach().cpu().numpy() # If we wanted to comput logits into numpy array, e.g. for acc or auc computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3d700-8cf5-4e90-82c6-65f002893b43",
   "metadata": {},
   "source": [
    "## Back to demonstrating how to process multiple strings at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db3ae6cc-0314-47f0-9bac-b0b5e1a08349",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2066, 8870, 1012,  102,    0,    0],\n",
       "        [ 101, 2079, 2017, 2066, 8870, 2205, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "  \"I like cats.\",\n",
    "  \"Do you like cats too?\",\n",
    "]\n",
    "model_inputs = tokenizer(data, padding=True, truncation=True, return_tensors='pt')\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "156dd5f5-ad4f-427d-a911-c7f455e6cc7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2066, 8870, 1012,  102,    0,    0],\n",
       "        [ 101, 2079, 2017, 2066, 8870, 2205, 1029,  102]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26e24dd7-d7ce-4887-b13b-ccd3e50ce99c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "288690c0-5d27-4135-aaf9-f5d140d8f614",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1097,  0.5827, -0.3432],\n",
       "        [-0.0794,  0.5943, -0.3126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**model_inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a61daad-e6b3-480d-b5d1-bd4579e49f5f",
   "metadata": {},
   "source": [
    "**Note:** 2x3 tensor as an output. This makes sense, as we passed two docs as an input, and the number of classes is 3."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
